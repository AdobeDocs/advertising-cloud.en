# FAQs About Surveys {#survey-faqs}

**What can I measure using Surveys by Ad Cloud?**

Surveys by Ad Cloud are best used as a media optimization and insight tool. A few test examples include: Creative comparison test, Audience comparison, Format/Retargeting test, Frequency test, etc. See "Planning & Consulting" page for more detail.

**What is the length of a Survey ad?**

Pre-Roll Survey ads run in a 15s ad slot. The survey unit will require respondents to answer questions in the order they are asked, they will not be able to skip around to answer questions.  If the user completes the survey before the 15s, they will go directly to the content they intended to watch.

**What survey type should I choose?**

For almost all campaigns Desktop Pre-Roll or Mobile surveys should be chosen. Since we cannot currently target cross-device it is best to evaluate how much of your overall media will be delivered on Desktop vs Mobile In-App. Whichever format will have the majority of delivery should be chosen for the survey. If there is an even split and a large enough budget, running identical Mobile and Desktop surveys is best and results can be combined post-survey. See setup page for more details.

**How many questions should I ask?**

We recommend 2 to 3 questions due to drop off rates. Additional questions can be included but media budget and survey budget should be considered. 

**What are the drop off rates between questions?**

General rule of thumb - estimate 50% will complete 2-question survey, 30% will complete 3-question survey, 20% will complete 4-question survey.

Simple is usually best. Consider shortening long answer choices and removing any repetitive or unnecessary wording (for example, change ""I would consider Brand X"" to ""Brand X""). For Likert scale measurement, we typically recommend simplifying answers [Yes, No, I'm not Sure] if possible.". Reminder there is a maximum of six answer choices and the tool does not allow for matrix style question / answer combinations or fill in the blank, only multiple choice. What's the best practice for formatting question answers?

**What do I do about negative lift and an imbalance of control and exposed responses?**

There are a few things you can do to troubleshoot for negative lift:

* Double check your set up to ensure there weren't any simple mistakes made when creating the survey or associated placements (Set-up checklist)

* Check your control group: Your control group should mirror targeting on your media, the only difference between these groups should be ad exposure. Targeting that is too broad (such as Adults 21+) or grouping multiple unrelated data segments can contribute to incorrect control groups

* Re-evaluate your test design Note: If you have 200-500 responses, running a survey longer is not a great strategy for changing results. Often, there is something else that needs fixing besides more responses.

**What is Brand Lift? And how should I be framing the results for my client?**

Brand Lift is measured by comparing a control group versus an exposed groups response to a question to determine if there was a positive impact after having seen the Brand’s Ad. When describing lift, you should always specify what you are measuring (i.e./ Brand lift in Awareness, Consideration, Purchase Intent).

**What are brand lift benchmarks?**

We do not currently aggregate brand lift benchmarks by category. This will depend on a lot of different factors. (brand, media budget, Seasonality, what you are measuring, i.e. awareness vs purchase) It is not uncommon for lift to be very small, especially for a brand that might be well known among the control group.

**Can we run non-English surveys? And will they scale?**

Different languages don’t require a custom build.  There should be no major differences if the survey set up is logical i.e., you’re targeting people who speak the language. While GDPR restrictions may play a role in overall scale, you should not run into any issues should you be seeing healthy scale on your standard media placements.

**Does a large sample size guarantee statistical significance at any level (i.e. 80%, 90%, etc.)?**

No. Overall increase in sample size may increase the opportunity for results to be statistically significant, but larger sample sizes do not guarantee statistically significant results.

**Why does a large sample size NOT guarantee statistical significance?**

Larger bases can allow for wider variances of response, especially when it comes to extremely specific survey questions.
